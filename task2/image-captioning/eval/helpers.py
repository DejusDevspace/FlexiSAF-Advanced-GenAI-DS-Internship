import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import torch
import csv
import torchvision.transforms as transforms
from PIL import Image
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from utils.datasets import FlickrDataset
from utils.captions import CaptionCollate
from models.captioner import ImageCaptioningModel

def load_model(checkpoint_path, device):
    """
    Load trained model from checkpoint.
    """
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    vocab = checkpoint['vocab']
    embed_size = checkpoint['embed_size']
    hidden_size = checkpoint['hidden_size']
    num_layers = checkpoint['num_layers']
    vocab_size = len(vocab)

    # Initialize model
    model = ImageCaptioningModel(
        embed_size=embed_size,
        hidden_size=hidden_size,
        vocab_size=vocab_size,
        num_layers=num_layers
    ).to(device)

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    return model, vocab


def generate_caption(image_path, model, vocab, device, max_length=20):
    """
    Generate caption for a single image.
    """
    # Image transformation
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
    ])

    # Load and preprocess image
    image = Image.open(image_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0).to(device)

    # Generate caption
    with torch.no_grad():
        caption_indices = model.generate_caption(image_tensor, max_length)

    # Convert indices to words
    caption_words = []
    for idx in caption_indices:
        word = vocab.itos[idx]
        if word == "<end>":
            break
        if word not in ["<start>", "<pad>"]:
            caption_words.append(word)

    caption = " ".join(caption_words)
    return caption, image

# def calculate_bleu(reference_captions, generated_caption):
#     """
#     reference_captions: list of actual captions
#     generated_caption: model's predicted caption
#     """
#     reference = [ref.split() for ref in reference_captions]
#     candidate = generated_caption.split()
#     score = sentence_bleu(reference, candidate)
#     return score

def calculate_bleu(image_path, generated_caption, captions_file='captions.txt'):
    """
    Calculate BLEU-4 score for a generated caption given an image path and a captions file.

    Args:
        image_path (str): Path to the image file (used to match in captions.txt).
        generated_caption (str): Caption generated by the model.
        captions_file (str): Path to the captions.txt file.

    Returns:
        float: BLEU-4 score.
    """
    # Extract image filename (e.g. '1000268201_693b08cb0e.jpg')
    image_name = os.path.basename(image_path)

    # Read reference captions for this image
    reference_captions = []
    with open(captions_file, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # skip header: image,caption
        for img, caption in reader:
            if img.strip() == image_name:
                reference_captions.append(caption.strip())

    if not reference_captions:
        print(f"[Warning] No reference captions found for {image_name}")
        return 0.0

    # Tokenize references and candidate
    references = [ref.split() for ref in reference_captions]
    candidate = generated_caption.split()

    # BLEU-4 with smoothing
    smoothie = SmoothingFunction().method4
    score = sentence_bleu(
        references,
        candidate,
        weights=(0.25, 0.25, 0.25, 0.25),
        smoothing_function=smoothie
    )

    return score
